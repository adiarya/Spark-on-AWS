{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:13:53.779442Z",
     "iopub.status.busy": "2025-08-31T06:13:53.779247Z",
     "iopub.status.idle": "2025-08-31T06:14:38.779371Z",
     "shell.execute_reply": "2025-08-31T06:14:38.778479Z",
     "shell.execute_reply.started": "2025-08-31T06:13:53.779419Z"
    },
    "kernelspec": {
     "display_name": "SparkMagic PySpark",
     "language": "python",
     "name": "pysparkkernel"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "python",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "pyspark",
     "pygments_lexer": "python3"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Read [/opt/ml/metadata/resource-metadata.json] from disk at 1756611008870351350\n",
      "INFO:root:Returning content of [/opt/ml/metadata/resource-metadata.json] updated at time 1756611008870351350\n",
      "INFO:root:Read [/opt/.sagemakerinternal/internal-metadata.json] from disk at 1756610999302310693\n",
      "/opt/conda/lib/python3.12/site-packages/sagemaker_studio_analytics_extension/resource/resource_metadata.py:1: UserWarning:\n",
      "\n",
      "pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read emr cluster(j-BMCH4XQC5JW5) details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker_studio_sparkmagic_lib.emr:Successfully read emr cluster(j-BMCH4XQC5JW5) details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating EMR connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1756611020725_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-2-1-226.us-west-2.compute.internal:20888/proxy/application_1756611020725_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-2-1-173.us-west-2.compute.internal:8042/node/containerlogs/container_1756611020725_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc3ff43cb434ddb83a5ae7b861bb030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:278267777be742d59f36789f6c9282f2:{\"LibraryVersion\": \"0.2.0\", \"Service\": \"emr\", \"Operation\": \"connect\", \"AccountId\": \"046078786454\", \"EventTimeStampMillis\": 1756620878774.57, \"Exception\": null, \"ExceptionString\": null, \"Error\": 0, \"Fault\": 0, \"Success\": 1, \"AuthType\": \"no-auth\", \"ConnectionProtocol\": \"http\", \"StackTrace\": null, \"OperationStartTimeMillis\": 1756620835314.8145, \"OperationEndTimeMillis\": 1756620878774.562, \"OperationDurationMillis\": 43459.74755859375, \"KernelName\": \"PySparkKernel\", \"ClusterId\": \"j-BMCH4XQC5JW5\", \"VerifyCertificate\": \"False\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"namespace\": \"sagemaker-analytics\", \"cluster_id\": \"j-BMCH4XQC5JW5\", \"error_message\": null, \"success\": true, \"service\": \"emr\", \"operation\": \"connect\"}\n"
     ]
    }
   ],
   "source": [
    "# Load the SageMaker Studio Analytics extension and connect to the EMR cluster\n",
    "# The first line loads the custom magics for SageMaker Studio Analytics.\n",
    "# The second line connects to the specified EMR cluster using the provided cluster ID\n",
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --verify-certificate False --cluster-id j-BMCH4XQC5JW5 --auth-type None --language python  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and Querying Data with SparkMagic PySpark\n",
    " \n",
    "In this notebook, we play around with data using SparkMagic PySpark while connected to an Amazon EMR cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Out the Session Info\n",
    " \n",
    "Since we're using the PySpark kernel, we can just use the %%info magic to see what's up with our current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:16:46.864394Z",
     "iopub.status.busy": "2025-08-31T06:16:46.863945Z",
     "iopub.status.idle": "2025-08-31T06:16:46.955809Z",
     "shell.execute_reply": "2025-08-31T06:16:46.955052Z",
     "shell.execute_reply.started": "2025-08-31T06:16:46.864367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'sagemaker_studio_analytics_spark_session_1df834982b7d4ed3896a2a9cc4ce568a', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1756611020725_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-2-1-226.us-west-2.compute.internal:20888/proxy/application_1756611020725_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-2-1-173.us-west-2.compute.internal:8042/node/containerlogs/container_1756611020725_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display current PySpark session information\n",
    "# The %%info magic command shows details about the current Spark session, such as version, configuration, and connection status.\n",
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and Querying the Data\n",
    " \n",
    "So, the PySpark Kernel gives us a general SQLContext, but here we're using HiveContext, which is more about Hive tables and stuff. SQLContext is more general, but HiveContext is great for Hive features like UDFs and indexing. \n",
    " \n",
    "When we connect to the EMR cluster, SparkContext and HiveContext are set up for us. We use HiveContext to run SQL queries on Hive tables and get the results as Spark dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use HiveContext to check out what databases and tables are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:17:32.191235Z",
     "iopub.status.busy": "2025-08-31T06:17:32.190959Z",
     "iopub.status.idle": "2025-08-31T06:17:37.521090Z",
     "shell.execute_reply": "2025-08-31T06:17:37.520375Z",
     "shell.execute_reply.started": "2025-08-31T06:17:32.191214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d764d332b353425bba73eb6eeb8fc0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "| default|adult_data|      false|\n",
      "+--------+----------+-----------+"
     ]
    }
   ],
   "source": [
    "# Query Hive databases and tables using HiveContext\n",
    "# Re-initialize sqlContext as a HiveContext to enable Hive-specific features.\n",
    "sqlContext = HiveContext(sqlContext)\n",
    "# List all databases in Hive.\n",
    "dbs = sqlContext.sql(\"show databases\")\n",
    "dbs.show()\n",
    "# List all tables in the current database.\n",
    "tables = sqlContext.sql(\"show tables\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the data from the 'adult_data' table and load it into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:17:46.715147Z",
     "iopub.status.busy": "2025-08-31T06:17:46.714871Z",
     "iopub.status.idle": "2025-08-31T06:17:47.480492Z",
     "shell.execute_reply": "2025-08-31T06:17:47.479680Z",
     "shell.execute_reply.started": "2025-08-31T06:17:46.715124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3b346d43774eb2b08fb9074696faf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the 'adult_data' Hive table and cache the result in a Spark dataframe.\n",
    "# Caching improves performance for repeated operations on this dataframe.\n",
    "adult_df = sqlContext.sql(\"select * from adult_data\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check out the shape of the dataframe and look at the first five rows to get a feel for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:22:55.244039Z",
     "iopub.status.busy": "2025-08-31T06:22:55.243731Z",
     "iopub.status.idle": "2025-08-31T06:22:56.038309Z",
     "shell.execute_reply": "2025-08-31T06:22:56.037664Z",
     "shell.execute_reply.started": "2025-08-31T06:22:55.244013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf32156df364e0f9376a61a43aad17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 15)\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: string (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: string (nullable = true)\n",
      " |-- capital_loss: string (nullable = true)\n",
      " |-- hours_per_week: string (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# View the shape of the dataframe (number of rows, number of columns)\n",
    "print((adult_df.count(), len(adult_df.columns)))\n",
    "# Show the first 5 rows of the dataframe\n",
    "adult_df.head(5)\n",
    "# Print the schema of the dataframe to see column types and structure\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the output easier to read, we convert the Spark dataframe to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:24:03.095115Z",
     "iopub.status.busy": "2025-08-31T06:24:03.094638Z",
     "iopub.status.idle": "2025-08-31T06:24:04.398753Z",
     "shell.execute_reply": "2025-08-31T06:24:04.397805Z",
     "shell.execute_reply.started": "2025-08-31T06:24:03.094940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae422aa291eb4b5f99e75f50a1a591b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age   workclass  fnlwgt  ...  hours_per_week  native_country  income\n",
      "0  age   workclass  fnlwgt  ...  hours_per_week  native_country  income\n",
      "1   25     Private  226802  ...              40   United-States   <=50K\n",
      "2   38     Private   89814  ...              50   United-States   <=50K\n",
      "3   28   Local-gov  336951  ...              40   United-States    >50K\n",
      "4   44     Private  160323  ...              40   United-States    >50K\n",
      "\n",
      "[5 rows x 15 columns]"
     ]
    }
   ],
   "source": [
    "# Convert the first 5 rows of the Spark dataframe to a Pandas dataframe for cleaner output.\n",
    "adult_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ML algorithms (like linear regression) need numeric features, but our dataset has a bunch of categorical columns like workclass, education, occupation, etc.\n",
    "\n",
    "We use StringIndexer and OneHotEncoder to turn those categorical columns into numeric ones. StringIndexer gives us label indexes, and OneHotEncoder turns those into binary vectors (so each category gets its own column with 0s and 1s).\n",
    " \n",
    "First, we use StringIndexer, then OneHotEncoder. You can check out the docs for [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) and [OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder) if you want more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:32:15.427796Z",
     "iopub.status.busy": "2025-08-31T06:32:15.427499Z",
     "iopub.status.idle": "2025-08-31T06:32:16.178092Z",
     "shell.execute_reply": "2025-08-31T06:32:16.177311Z",
     "shell.execute_reply.started": "2025-08-31T06:32:15.427770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da13d321fca74fb1b46ae84a012c17a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert categorical variables to numeric using StringIndexer and OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "# List of categorical columns to be encoded\n",
    "categorical_variables = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "# Create StringIndexer objects for each categorical column\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_idx\") for column in categorical_variables]\n",
    "# Create OneHotEncoder objects for each indexed column\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_ohe\") for col in categorical_variables]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use VectorAssembler to combine all those one-hot encoded columns into a single column that holds an array of values.\n",
    "\n",
    "If you want to know more about VectorAssembler, check out the [docs](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:32:20.263760Z",
     "iopub.status.busy": "2025-08-31T06:32:20.263473Z",
     "iopub.status.idle": "2025-08-31T06:32:20.508950Z",
     "shell.execute_reply": "2025-08-31T06:32:20.508215Z",
     "shell.execute_reply.started": "2025-08-31T06:32:20.263738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a22c5d235c48f3a5a6d0814410f299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use VectorAssembler to combine all one-hot encoded categorical features into a single vector column\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[col+\"_ohe\" for col in categorical_variables],\n",
    "    outputCol=\"categorical-features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are just a way to chain together a bunch of transformers and estimators so we don't have to do each step by hand every time.\n",
    "\n",
    "We set up a pipeline with all our steps, fit it to the data, and then use it to transform the dataset. If you want to read more about pipelines, check out the [docs](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:35:21.720173Z",
     "iopub.status.busy": "2025-08-31T06:35:21.719899Z",
     "iopub.status.idle": "2025-08-31T06:35:31.048890Z",
     "shell.execute_reply": "2025-08-31T06:35:31.048185Z",
     "shell.execute_reply.started": "2025-08-31T06:35:21.720150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2dc07318174bd2bc597528e2f30da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build and apply a PySpark pipeline to automate the transformations\n",
    "from pyspark.ml import Pipeline\n",
    "# Define the pipeline with all indexers, encoders, and the assembler\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "# Fit the pipeline to the data to create a PipelineModel\n",
    "pipelineModel = pipeline.fit(adult_df)\n",
    "# Transform the dataset using the fitted pipeline model\n",
    "adult_df = pipelineModel.transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check out all the new columns that get created after running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:35:34.679707Z",
     "iopub.status.busy": "2025-08-31T06:35:34.679418Z",
     "iopub.status.idle": "2025-08-31T06:35:34.715860Z",
     "shell.execute_reply": "2025-08-31T06:35:34.715137Z",
     "shell.execute_reply.started": "2025-08-31T06:35:34.679684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e3c4f1c5b145f2912af6914fa53b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education_num: string (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital_gain: string (nullable = true)\n",
      " |-- capital_loss: string (nullable = true)\n",
      " |-- hours_per_week: string (nullable = true)\n",
      " |-- native_country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- workclass-index: double (nullable = false)\n",
      " |-- education-index: double (nullable = false)\n",
      " |-- marital_status-index: double (nullable = false)\n",
      " |-- occupation-index: double (nullable = false)\n",
      " |-- relationship-index: double (nullable = false)\n",
      " |-- race-index: double (nullable = false)\n",
      " |-- sex-index: double (nullable = false)\n",
      " |-- native_country-index: double (nullable = false)\n",
      " |-- workclass-index-encoded: vector (nullable = true)\n",
      " |-- occupation-index-encoded: vector (nullable = true)\n",
      " |-- marital_status-index-encoded: vector (nullable = true)\n",
      " |-- education-index-encoded: vector (nullable = true)\n",
      " |-- native_country-index-encoded: vector (nullable = true)\n",
      " |-- race-index-encoded: vector (nullable = true)\n",
      " |-- relationship-index-encoded: vector (nullable = true)\n",
      " |-- sex-index-encoded: vector (nullable = true)\n",
      " |-- categorical-features: vector (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Print the schema of the transformed dataframe to review new columns created by the pipeline\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the transformations, we end up with a single column that has an array with all the encoded categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T06:36:56.682196Z",
     "iopub.status.busy": "2025-08-31T06:36:56.681896Z",
     "iopub.status.idle": "2025-08-31T06:36:57.943344Z",
     "shell.execute_reply": "2025-08-31T06:36:57.942443Z",
     "shell.execute_reply.started": "2025-08-31T06:36:56.682170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d58f00f714f088c7b725c19a8be56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|categorical-features                                           |\n",
      "+---------------------------------------------------------------+\n",
      "|(86,[79],[1.0])                                                |\n",
      "|(86,[0,11,24,36,47,52,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(86,[0,7,23,40,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[2,16,23,42,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(86,[0,8,23,36,45,52,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[3,8,24,37,47,51,57,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,13,24,33,46,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(86,[3,7,24,37,48,52,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[1,15,23,31,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(86,[0,8,24,33,48,51,57,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,14,23,32,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(86,[0,7,23,36,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[6,9,23,34,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,7,24,34,46,51,57,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[3,7,23,37,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,7,23,36,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,10,23,30,45,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(86,[4,8,24,33,47,51,56,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,7,23,34,49,51,57,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(86,[0,7,27,36,48,51,57,58],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Show the 'categorical-features' column, which contains the combined one-hot encoded features as a vector\n",
    "adult_df.select('categorical-features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to encode the target label too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T07:08:03.586822Z",
     "iopub.status.busy": "2025-08-31T07:08:03.586521Z",
     "iopub.status.idle": "2025-08-31T07:08:04.337783Z",
     "shell.execute_reply": "2025-08-31T07:08:04.337010Z",
     "shell.execute_reply.started": "2025-08-31T07:08:03.586799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb49f876fc64e14a3008d09b28d45c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the target label ('income') as a numeric column called 'label' using StringIndexer\n",
    "indexer = StringIndexer(inputCol='income', outputCol='label')\n",
    "adult_df = indexer.fit(adult_df).transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Out the Spark UI\n",
    " \n",
    "Here we use the Spark UI to see how our jobs are running and check out performance details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the current Spark session info again just to make sure everything is still working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T07:08:16.762623Z",
     "iopub.status.busy": "2025-08-31T07:08:16.762154Z",
     "iopub.status.idle": "2025-08-31T07:08:16.797159Z",
     "shell.execute_reply": "2025-08-31T07:08:16.796255Z",
     "shell.execute_reply.started": "2025-08-31T07:08:16.762595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'sagemaker_studio_analytics_spark_session_1df834982b7d4ed3896a2a9cc4ce568a', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1756611020725_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-2-1-226.us-west-2.compute.internal:20888/proxy/application_1756611020725_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-2-1-173.us-west-2.compute.internal:8042/node/containerlogs/container_1756611020725_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display current Spark session information again to monitor the session after transformations\n",
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find links for the Spark UI and Driver log (though the Driver log link doesn't work for us). The Spark UI link is generated when we connect to the EMR cluster. We use it to check out our Spark jobs in a browser and see how things are running.\n",
    "\n",
    "Some cool stuff in the Spark server:\n",
    "- The Jobs tab shows all the Spark jobs and their status.\n",
    "- The Event Timeline gives a visual of the different stages.\n",
    "- Completed Jobs are listed in a table, and you can click on them for more details.\n",
    "- The DAG Visualization lets you see the tasks and dig into the details for each stage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    " \n",
    "That's it for this notebook! To move on, just close this file and head back to the lab session for the conclusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
